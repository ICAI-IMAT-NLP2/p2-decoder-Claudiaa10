{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory Practice 2: Generating names with a decoder-only transformer\n",
    "\n",
    "In this notebook, we will generate new names using a Transformer decoder model. This simple text generation task captures the essential components of language modeling, applied to a small, manageable dataset.\n",
    "\n",
    "More specifically, you will be training autoregressive, character-level, decoder-only language model. You will feed it a database of names, and the model will generate new name ideas that all sound name-like, but are not already existing names. \n",
    "\n",
    "First, you'll train the model. After training, you'll generate names using pure random sampling as your decoding strategy. Pure random sampling doesn't always work well, so you'll also learn to tweak the temperature parameter when sampling, to better control your generation output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import the data processing and model modules\n",
    "from data_processing import load_and_preprocess_data, CharTokenizer, NameDataset, collate_fn\n",
    "from train import train \n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cpu\")# torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "\n",
    "In this section, you will take a look at the data. You should be familiar with it, but it is used a little different now that we are training a decoder model. Play with it and answer the questions at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "First 10 names: ['-maria carmen.', '-antonio.', '-maria.', '-manuel.', '-jose.', '-francisco.', '-david.', '-carmen.', '-juan.', '-javier.']\n"
     ]
    }
   ],
   "source": [
    "data_filepath = \"../data/nombres_raw.txt\"  # Replace with your actual file path\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "start_token = \"-\"\n",
    "end_token = \".\"\n",
    "batch_size = 64\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "names = load_and_preprocess_data(data_filepath, alphabet, start_token, end_token)\n",
    "print(\"First 10 names:\", names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding names...\n",
      "-maria carmen.\n",
      "First 10 encoded names: [[1, 16, 4, 21, 12, 4, 3, 6, 4, 21, 16, 8, 17, 2], [1, 4, 17, 23, 18, 17, 12, 18, 2], [1, 16, 4, 21, 12, 4, 2], [1, 16, 4, 17, 24, 8, 15, 2], [1, 13, 18, 22, 8, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = CharTokenizer(alphabet, start_token=start_token, end_token=end_token)\n",
    "\n",
    "# Encode names\n",
    "print(\"Encoding names...\")\n",
    "encoded_names = [tokenizer.encode(name) for name in names]\n",
    "print(\"First 10 encoded names:\", encoded_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the ```CharTokenizer``` to encode your name. What is the result of encoding your name?\n",
    ">>> - claudia\n",
    "\n",
    ">>> [1, 6, 15, 4, 24, 7, 12, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First item in batch:\n",
      "Input: tensor([ 1, 17,  8, 15, 22, 18, 17,  3, 21, 24,  5,  8, 17,  0,  0,  0,  0,  0,\n",
      "         0])\n",
      "Target: tensor([17,  8, 15, 22, 18, 17,  3, 21, 24,  5,  8, 17,  2,  0,  0,  0,  0,  0,\n",
      "         0])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = NameDataset(encoded_names)\n",
    "\n",
    "# Create data loader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Obtain a batch from data loader\n",
    "batch = next(iter(data_loader))\n",
    "\n",
    "# LetÂ´s check one item from batch\n",
    "print(\"First item in batch:\")\n",
    "print(\"Input:\", batch[0][0])\n",
    "print(\"Target:\", batch[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can you obtain the target tensor from the input? Does this make sense for an autorregressive prediction such as the one of the Decoder-only model?\n",
    ">>> Write your answer here\n",
    "\n",
    "- What is the tensor value for the start token? And for the end token? And for the padding token?\n",
    ">>> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model and tokenizer\n",
    "Here you will train the decoder model. Feel free to change the hyperparameters of the model in the ```model_params``` dictionary. Be careful with your computational resources!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Starting training...\n",
      "Epoch [1/20], Training Loss: 2.8489\n",
      "Epoch [1/20], Validation Loss: 2.7460\n",
      "Epoch [2/20], Training Loss: 2.7332\n",
      "Epoch [2/20], Validation Loss: 2.7299\n",
      "Epoch [3/20], Training Loss: 2.7221\n",
      "Epoch [3/20], Validation Loss: 2.7213\n",
      "Epoch [4/20], Training Loss: 2.7172\n",
      "Epoch [4/20], Validation Loss: 2.7180\n",
      "Epoch [5/20], Training Loss: 2.7137\n",
      "Epoch [5/20], Validation Loss: 2.7154\n",
      "Epoch [6/20], Training Loss: 2.7112\n",
      "Epoch [6/20], Validation Loss: 2.7135\n",
      "Epoch [7/20], Training Loss: 2.7098\n",
      "Epoch [7/20], Validation Loss: 2.7117\n",
      "Epoch [8/20], Training Loss: 2.7085\n",
      "Epoch [8/20], Validation Loss: 2.7110\n",
      "Epoch [9/20], Training Loss: 2.7081\n",
      "Epoch [9/20], Validation Loss: 2.7101\n",
      "Epoch [10/20], Training Loss: 2.7076\n",
      "Epoch [10/20], Validation Loss: 2.7096\n",
      "Epoch [11/20], Training Loss: 2.7060\n",
      "Epoch [11/20], Validation Loss: 2.7094\n",
      "Epoch [12/20], Training Loss: 2.7063\n",
      "Epoch [12/20], Validation Loss: 2.7086\n",
      "Epoch [13/20], Training Loss: 2.7060\n",
      "Epoch [13/20], Validation Loss: 2.7091\n",
      "Epoch [14/20], Training Loss: 2.7058\n",
      "Epoch [14/20], Validation Loss: 2.7095\n",
      "Epoch [15/20], Training Loss: 2.7053\n",
      "Epoch [15/20], Validation Loss: 2.7103\n",
      "Epoch [16/20], Training Loss: 2.7059\n",
      "Epoch [16/20], Validation Loss: 2.7087\n",
      "Epoch [17/20], Training Loss: 2.7049\n",
      "Epoch [17/20], Validation Loss: 2.7083\n",
      "Epoch [18/20], Training Loss: 2.7054\n",
      "Epoch [18/20], Validation Loss: 2.7083\n",
      "Epoch [19/20], Training Loss: 2.7053\n",
      "Epoch [19/20], Validation Loss: 2.7085\n",
      "Epoch [20/20], Training Loss: 2.7052\n",
      "Epoch [20/20], Validation Loss: 2.7081\n"
     ]
    }
   ],
   "source": [
    "# Define training hyper parameters\n",
    "model_save_dir = \"runs\"\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "model_params = {\n",
    "    \"d_model\": 64,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"intermediate_size\": 128,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"max_position_embeddings\": tokenizer.vocab_size # Do not touch this\n",
    "}\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Call the train function\n",
    "model = train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    model_save_dir=model_save_dir,\n",
    "    model_params=model_params,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Names\n",
    "Now we are ready to generate new names. Fill the function below and start playing around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prefix: str = \"\",\n",
    "    start_token: str = \"-\",\n",
    "    end_token: str = \".\",\n",
    "    max_length: int = 20,\n",
    "    temperature: float = 1.0,\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a new name using the trained model, optionally starting with a given prefix.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained Transformer model.\n",
    "        tokenizer (CharTokenizer): The tokenizer.\n",
    "        prefix (str): Optional prefix string to start the name.\n",
    "        start_token (str): The start token character.\n",
    "        end_token (str): The end token character.\n",
    "        max_length (int): Maximum length of the generated name (excluding prefix length).\n",
    "        temperature (float): Sampling temperature. Higher values increase randomness.\n",
    "        device (torch.device): Device to perform computation on.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated name.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_token_id = tokenizer.char2idx[start_token]\n",
    "    end_token_id = tokenizer.char2idx[end_token]\n",
    "\n",
    "    # TODO: Encode the prefix\n",
    "    prefix_ids = [tokenizer.char2idx[character] for character in prefix]\n",
    "\n",
    "    # TODO: Initialize the input with the start token and the prefix\n",
    "    generated_ids = torch.tensor([start_token_id] + prefix_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # TODO: Get model predictions\n",
    "            logits = model(generated_ids)[:, -1, :] / temperature\n",
    "            \n",
    "            # TODO: Apply softmax to get probabilities\n",
    "            next_token_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # TODO: Sample the next token\n",
    "            next_token_id  = torch.multinomial(next_token_probs, num_samples=1)\n",
    "\n",
    "            # TODO: Append the new token to the sequence\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "            # Stop if end token is generated\n",
    "            if next_token_id.item() == end_token_id:\n",
    "                break\n",
    "\n",
    "    # TODO: Decode the generated token IDs to a string, excluding start and end tokens\n",
    "    generated_sequence = generated_ids.squeeze().tolist()\n",
    "    generated_sequence = [tokenizer.idx2char[idx] for idx in generated_sequence]\n",
    "    # TODO: Decode the name\n",
    "    generated_name =  \"\".join([c for c in generated_sequence if c not in {start_token, end_token}])\n",
    "\n",
    "    return generated_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_names = 5          # Number of names to generate\n",
    "max_length = 20         # Maximum length of each generated name\n",
    "temperature = 1.0       # Sampling temperature \n",
    "start_token = \"-\"       # Start token character (used during training)\n",
    "end_token = \".\"         # End token character (used during training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Names:\n",
      "\n",
      "in\n",
      "t\n",
      "k\n",
      "\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "# Generate names\n",
    "print(\"Generated Names:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Temperature Parameter in Language Generation\n",
    "\n",
    "The **temperature** parameter $T$ adjusts the randomness of text generated by language models by scaling the logits (model outputs before softmax).\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "Given logits $z_i$ for each token $i$, the probability $p_i$ of selecting token $i$ is calculated using the softmax function:\n",
    "\n",
    "\\begin{align*}\n",
    "p_i = \\frac{\\exp\\left(\\frac{z_i}{T}\\right)}{\\sum_{j} \\exp\\left(\\frac{z_j}{T}\\right)}\n",
    "\\end{align*}\n",
    "\n",
    "- *When $T = 1$*: The probabilities remain unchanged.\n",
    "- *When $T < 1$*: The distribution becomes sharper; higher-probability tokens are favored. You should expect names to look more like the \"typical\" names encountered in the dataset.\n",
    "- *When $T > 1$*: The distribution flattens; lower-probability tokens are more likely. You should expect names to look more \"exotic\" or \"creative\", since less probable characters are being sampled to continue the previously generated ones.\n",
    "\n",
    "### Impact on Token Probabilities\n",
    "\n",
    "Suppose we have logits for three tokens:\n",
    "\n",
    "- $z_A$ = 2.0\n",
    "- $z_B$ = 1.0\n",
    "- $z_C$ = 0.5\n",
    "\n",
    "##### At $T = 1.0$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.659\\\\\n",
    "p_B &= \\frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.242\\\\\n",
    "p_C &= \\frac{e^{0.5}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.099\n",
    "\\end{align*}\n",
    "\n",
    "##### At $T = 0.5$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0 / 0.5}}{e^{2.0 / 0.5} + e^{1.0 / 0.5} + e^{0.5 / 0.5}} = \\frac{e^{4.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.843\\\\\n",
    "p_B &= \\frac{e^{2.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.114\\\\\n",
    "p_C &= \\frac{e^{1.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.043\n",
    "\\end{align*}\n",
    "\n",
    "- **Observation**: Lower $T$ increases the dominance of the highest logit.\n",
    "\n",
    "##### At $T = 1.5$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0 / 1.5}}{e^{2.0 / 1.5} + e^{1.0 / 1.5} + e^{0.5 / 1.5}} = \\frac{e^{1.333}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.490\\\\\n",
    "p_B &= \\frac{e^{0.667}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.282\\\\\n",
    "p_C &= \\frac{e^{0.333}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.228\n",
    "\\end{align*}\n",
    "\n",
    "- **Observation**: Higher $T$ increases the probabilities of less likely tokens.\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "- **Low Temperature ($T < 1$)**:\n",
    "  - **Sharper Distribution**: Model is confident; outputs are more predictable.\n",
    "  - **Use Case**: When coherence is crucial.\n",
    "\n",
    "- **High Temperature ($T > 1$)**:\n",
    "  - **Flatter Distribution**: Model explores more options; outputs are diverse.\n",
    "  - **Use Case**: When creativity is desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Names with temperature=1.5:\n",
      "\n",
      "dtbrd \n",
      "aiengthhhactii\n",
      "gtrgaof\n",
      "gdsi\n",
      "hzttaaiiontgzn\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.5  # High randomness\n",
    "print(f\"Generated Names with temperature={temperature}:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Names with temperature=0.75:\n",
      "\n",
      "s\n",
      "m\n",
      "bz\n",
      "d\n",
      "j\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.75  # Low randomness\n",
    "print(f\"Generated Names with temperature={temperature}:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z\n",
      "zaduo\n",
      "z\n",
      "z\n",
      "zelzt\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.0  # Default randomness\n",
    "prefix = \"z\"  # Prefix to start the names with\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        prefix=prefix,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entornovirtual1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
